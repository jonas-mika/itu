---
title: "Applied Statistics 2021 - Exercises 10"
output:
  pdf_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Maximum likelihood estimator for geometric random variables (Theory)

The geometric random variable, as presented in the textbook, has the
following probability mass function $$
   Pr[X = k] = (1-p)^{k-1}\cdot p
$$ which can be described as the probability of requiring $k$ trials to
obtain the first success in a sequence of Bernoulli trials. For this
random variable, we have seen that the maximum likelihood estimator for
the parameter $p$ is $$
  \hat{p} = \frac{n}{\sum_{i=0}^n x_i} = \frac{1}{\bar{x}}
$$

However, in some contexts [^1] a slightly different definition of
geometric random variable is used: $$
  Pr[X = k] = (1-p)^k\cdot p
$$This second formulation can be described as the probability of
experiencing $k$ consecutive failures before the first success.

[^1]: Including the R implementation of the geometric random
    distribution

We shall see, with this exercise, that this small change leads to a
different maximum likelihood estimator for $p$!

a.  Derive the loglikelihood function $\ell(p)$
b.  Compute the derivative $\ell'(p)$ of the loglikelihood function
c.  Show that the maximum likelihood estimator for $p$ is $$
      \hat{p} = \frac{n}{n+\sum_{i=0}^n{x_i}}
    $$

Therefore, *pay attention* to the distribution you are dealing with,
always read carefully the definitions and the documentation!

**A. Deriving Loglikelihood Function**

In order to find the loglikelihood function, we need to first compute
the likelihood function for the variation of the geometric distribution,
where the probability density function evaluated at some point $k$
yields the probability of experiencing $k$ failures before the first
success. We can conclude for some discrete random variable

$$
X \sim Geo(p) \implies P(X=k) = (1-p)^k \cdot p
$$

# 2. Maximum likelihood estimators for the Pareto distribution (Theory)

The Pareto distribution is used in a wide variety of contexts, ranging
from describing the size of meteorites to the error rates of disk
drives. The expression of the probability density function of the Pareto
distribution is $$
  f(x) = \frac{\alpha}{x^{\alpha + 1}} \quad\text{ for } x \ge 1
$$ Given the numerous applications, it is important to be able to
estimate the value of $\alpha$ from random samples. In this exercise you
will derive a maximum likelihood estimator for $\alpha$.

a.  Derive the loglikelihood function $\ell(\alpha)$
b.  Compute the derivative $\ell'(\alpha)$ of the loglikelihood function
c.  Derive the maximum likelihood estimator for $\alpha$

# 3. Linear models (Theory)

In some situations we may know that the linear model should have some
peculiarities, like having no slope, or having intercept equals to
zero[^2]. Answer to the two following separate questions (i.e. the
answer to one doesn't depend on the answer to the other). Let $U_i$ be
random variables with expectation zero and variance $\sigma^2$.

[^2]: For instance we may know that when one quantity of the bivariate
    dataset is 0 then the other *must* be zero.

a.  Consider the case $\alpha=0$. The model then becomes
    $Y_i = \beta x_i + U_i$, for $i=1,2,\dots,n$. Find the least squares
    estimate $\hat{\beta}$ for $\beta$.
b.  Consider the case $\beta=0$. The model is then $Y_i = \alpha + U_i$,
    for $i=1,2,\dots,n$. Find the least squares estimate $\hat{\alpha}$
    for $\alpha$.

# 4. Maximum likelihood estimator for geometric random variables (R)

In exercise 1, you did show that the geometric distribution defined as
$$
  Pr[X=k] = (1-p)^k \cdot p
$$ has the following maximum likelihood estimator for $p$: $$
  \hat{p}^* = \frac{n}{n+\sum_{i=0}^n{x_i}}
$$ This definition of geometric random variable is the one use by R, as
state at the beginning of the "Details" section of `help(rgeom)`. In
this exercise you will verify that, in this case, using the inverse of
the sample mean as the estimator for $p$ leads to heavily biased
estimations.

Let $n=200$. First of all, define a function `estimate_p` that, given
the realization of a random sample of $n$ elements it returns the
estimate of $p$ using the estimator
$\hat{p}^* = \frac{n}{n+\sum_{i=0}^n{x_i}}$.

Then, define $p=0.3$, and take a random sample of $n$ elements using the
`rgeom` function. From this random sample, estimate $p$ using first the
estimator $\hat{p} = \frac{1}{\bar{x}}$ and then using the estimator
$\hat{p}^* = \frac{n}{n+\sum_{i=0}^n{x_i}}$. Compute the two values
$\hat{p} - p$ and $\hat{p}^* - p$. What do the resulting numbers
suggest?

Repeat the above sampling and estimation procedure 1000 times,
accumulating the values $\hat{p} - p$ and $\hat{p}^* - p$ in two
separate lists. Plot the two distributions, possibly overlaying them on
the same plot. What can you conclude by observing the plot?

```{r}
maximum.likelihood.estimator.p <- function(n) {
    samples <- rgeom(n, p=.3)
    return (n / (n + sum(samples)))
}

bad.estimator <- function(n) {
    samples <- rgeom(n, p=.3)
    return (1 / mean(samples))
}

ml.estimations <- c()
bad.estimations <- c()

for (i in 1:1000) {
    ml.estimations <- c(ml.estimations, maximum.likelihood.estimator.p(500))
    bad.estimations <- c(bad.estimations, bad.estimator(500))
}
plot(density(ml.estimations), xlim=c(.2, .5),
     main="Distribution from Different Estimators")
lines(density(bad.estimations), col='blue')
abline(v=.3, col='red')
legend(.4,35, legend=c("Max. Likelihood Estimator", "Bad Estimator", "Real p"),
       col=c("black", "blue", "red"), lty=1:2, cex=0.8)
#print(mean(estimations))
#print(var(estimations))
```

# 5. Linear regression model and residuals (R)

Let us take a look at the `Cars93 (MASS)` data set.

(a) Plot the mileage `MPG.highway` in the function of `Horsepower`.
    Compute the least-squares estimate for the regression line and add
    it to the plot.

(b) What is the predicted mileage for a car with 225 horsepower?

(c) Compute and plot the residuals in the function of horsepower. On the
    basis of the residuals, is the linear model assumption reasonable?

```{r}
require('MASS')
plot(MPG.highway ~ Horsepower, data=Cars93,
     main="Scatter of Horsepower to Fuel Consumption (Miles per Gallon) on Highways", 
     xlab="Horsepower", 
     ylab="MPG (Miles per Gallon) on Highway")
model <- lm(MPG.highway ~ Horsepower, data=Cars93)
abline(model, col='red')
```

```{r}
# the average mileage for all carrs with horsepower equal to 225
horsepower.225 <- subset(Cars93, Horsepower==225) # only one car
mean(horsepower.225$MPG.highway)
```

```{r}
# residualss (distances between datapoints from linear regression model)
plot(residuals(model))
abline(h=0, col='green')
```

# 6. Diamond prices (R)

The `diamonds` dataset[^3] (available by loading the `UsingR` package)
contains prices of more than 50000 diamonds, along with other
information like the carats, the clarity of the diamond, the cut, etc...

[^3]: Note that this time the name is plural rather than singular. In a
    past exercise we used the `diamond` dataset, which containts far
    less information

1.  First of all, plot the scatterplot of diamond prices versus their
    carats. Can you spot a pattern?

2.  Try to add some color, to see if some patterns unveil! In
    particular, plot the price against the carats, with a different
    color for each level of the `clarity` column. Does the resulting
    plot reveal some structure of the dataset?

3.  Train a linear regression model on the entire dataset, and plot the
    resulting regression line on top of the previous plot. In your
    opinion, does it represent summarise accurately the data?

4.  Try to plot a regression line for some subsets of the dataset
    defined by clarity levels (e.g. all diamonds with clarity `"I1"`)
    and plot them. Do they provide a better fit for the subsets of data?

5.  Using the linear models built in the previous step, predict the
    price of two diamonds of `1.5` carats: the first with clarity
    `"I1"`, the other with clarity `"IF"`. Also predict the price of a
    `1.5` carats diamond using the model trained on the entire dataset.
    Comparing your results with the previous plots, are the models
    trained taking into account the clarity more powerful in their
    prediction?

```{r}
# scatter
require('UsingR')

sample <- diamonds
#sample <- diamonds[sample(nrow(diamonds), nrow(diamonds)*.05), ] # sample 5% of original dataset

plot(price ~ carat, data=sample,
     main='Scatterplot of Carat to Price of Diamonds', 
     xlab='Carat', 
     ylab='Price')
model <- lm(price ~ carat, data=sample)
abline(model, col='red')
```

```{r}
# scatter with coloring for categorical variable
ggplot(sample, aes(x = carat, y = price, color = clarity)) + 
geom_point(position = position_dodge(width = 0.4)) + 
#stat_summary(fun.data=mean_cl_normal) + 
geom_smooth(method='lm') + 
ggtitle('Scatterplot of Carat to Prices of Diamonds in Different Clarities') +
xlab('Carat') + ylab('Price')

```

```{r}
# predicting prices using linear models for different classes of diamonds based on the carat
clar1 <- subset(diamonds,  clarity=='I1')
clar2 <- subset(diamonds, clarity=='IF')

model1 <- lm(price ~ carat, data=clar1)
model2 <- lm(price ~ carat, data=clar2)

predict(model1, newdata=data.frame('carat' = c(1.5)))
predict(model2, newdata=data.frame('carat' = c(1.5)))
```
