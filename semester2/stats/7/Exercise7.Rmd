---
title: "Exercise 7"
subtitle: "Applied Statistics, IT University of Copenhagen"
output:
  pdf_document: default
documentclass: article
classoption: a4paper
references:
- id: Verzani2014
  type: book
  title: "Using R for Introductory Statistics"
  author:
  - family: Verzani
    given: John
  issued:
    year: 2014
  publisher: CRC Press
---

# Preparation

* Read pages 72--77, 236--240 from @Verzani2014.

# Problems 

T=Theoretical Exercise, R=R-Exercise

## 1. Sum of Bernoulli Distributed Variables (T)

Let $X$ and $Y$ be two independent random variables where $X \sim \mathrm{Ber}(p)$ and $Y \sim \mathrm{Ber}(q)$. Let $Z=X+Y$. Investigate how $Z$ is distributed by deriving the probability mass function for $Z$.

## 2. Casino La Cella Fortuna (T)

The casino La bella Fortuna is for sale and you think you might want to buy it, but you want to know how much money you are going to make. All the present owner can tell you is that the roulette game Red or Black is played about 1000 times a night, 365 days a year. Each time it is played you have probability 19/37 of winning the player’s bet of 1 EUR and probability 18/37 of having to pay the player 1 EUR.

Explain in detail why the law of large numbers can be used to determine the
income of the casino, and determine how much it is.

## 3. Central Limit Theorem (T)

Let $X_1, X_2,\ldots$ be a sequence of independent $N(0, 1)$ distributed random
variables. For $n = 1,2, \ldots$, let $Y_n$ be the random variable, defined by
$Y_n = X_1^2 + \ldots + X_n^2$.

(a) Show that $E[X_i^2]=1.$

(b) One can show—using integration by parts—that $E[X_i^4]=3$. Deduce from this that $\mathrm{Var}(X_i^2)=2.$

(c) Use the central limit theorem to approximate $P(Y_{100} > 110)$.


## 4. Sum of Random Variables (R)

Assume you have $n$ dice you throw simultaneously. The sum of outcomes can be characterised by the formula $X=X_1+X_2+...+X_n$. Simulate the sum for thousand times using a couple of different values for $n$. Plot the histogram of the outcomes (`hist`) for each values of $n$ you chose. What is the distribution like when $n=2$? What can you see when you increase $n$?  

## 5. Michelson Experiment (R)

In 1879, A.A. Michelson made a famous experiment to determine the speed of light, the data set is available as `Michelson(HistData)`. The velocity estimates are $v=299000 \mathrm{km}/\mathrm{s} + \delta v$ where $\delta v$ is the velocity value saved in the data set. 

(a) Study the examples shown by typing '?Michelson'.

(b) Make density plot of the measurements together with data together with the individual estimates and true speed of light.

(c) Assume that the velocity estimates are independent and identically distributed. Compute an estimate for the speed of light and compare to the true value known today. Add the estimate to the plot.

(d) Use Chebychev's inequality to find out an upper bound for the probability that the speed of light was equal or further away from the value known today. 

(e) After looking closer at the data set do you have any reservations about the assumptions made above?

## 6. Central Limit Theorem (R)

As a simulated experiment, draw $n$ independent samples from Gamma distribution with shape parameter $a=7.1$ and scale parameter $s=1.44$, that is, $X_i\sim \Gamma(a,s)$, $i=1,2,\ldots,n$. Repeat the experiment $m$ times 

(a) Use the central limit theorem to compare the sample mean and variance of the standardised average to the theoretic values.

(b) Plot the histogram for the standardized average. How far are you from the theoretic density that you will get on the limit?
